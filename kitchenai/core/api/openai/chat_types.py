from typing import List, Optional, Union, Dict, Any
from pydantic import BaseModel, Field
from enum import Enum

#whisk related info
from whisk.kitchenai_sdk.schema import SourceNodeSchema

# ------------------------------------------
# 1) Request Models
# ------------------------------------------

class ChatRole(str, Enum):
    developer = "developer"  # Newer "o1" models
    system = "system"        # Older models
    user = "user"
    assistant = "assistant"
    tool = "tool"
    function = "function"    # Deprecated, replaced by tool
    # etc., add more roles if needed

class AssistantFunctionCall(BaseModel):
    """Deprecated in the spec, but shown for illustration."""
    name: str
    arguments: str

class ChatMessage(BaseModel):
    """
    Generic message type that can represent developer/system/user/assistant/tool messages.
    Many fields are optional and relevant only for certain roles.
    """
    role: ChatRole
    content: Optional[Union[str, List[str]]] = None
    refusal: Optional[str] = None
    name: Optional[str] = None
    # These fields are more relevant for the assistant role:
    audio: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    function_call: Optional[AssistantFunctionCall] = None  # Deprecated approach

class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="ID of the model to use.")
    messages: List[ChatMessage] = Field(..., description="A list of messages in the conversation so far.")
    stream: Optional[bool] = Field(False, description="If True, partial message deltas are streamed.")
    n: Optional[int] = Field(1, description="Number of chat completion choices to generate.")
    temperature: Optional[float] = Field(1.0, description="Sampling temperature, between 0 and 2.")
    top_p: Optional[float] = Field(1.0, description="Nucleus sampling parameter.")
    frequency_penalty: Optional[float] = Field(0.0, description="Penalize repeated tokens.")
    presence_penalty: Optional[float] = Field(0.0, description="Penalize repeated tokens if present in text so far.")
    max_completion_tokens: Optional[int] = Field(None, description="Max tokens for the completion (o1 models).")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Metadata to be stored with the request.")
    version: Optional[str] = Field(None, description="Version of the request.")
    namespace: str = Field(..., description="Namespace for the request.")
    # Many more optional fields from spec: store, reasoning_effort, logit_bias, stop, etc.
    # Simply add them as needed.

# ------------------------------------------
# 2) Response Models
# ------------------------------------------




# -------------------------
# 1. Sub-models
# -------------------------

class ChatCompletionLogProbs(BaseModel):
    """
    If the user sets 'logprobs': true, or top_p, or logit_bias, etc.,
    the API may return log probability details.
    You can expand these fields to match exactly what your system returns.
    """
    tokens: Optional[List[str]] = None
    token_logprobs: Optional[List[float]] = None
    top_logprobs: Optional[List[Dict[str, float]]] = None
    text_offset: Optional[List[int]] = None


class ChatCompletionMessage(BaseModel):
    """
    Represents a single message object in the response's 'choices'.
    Typically, role='assistant' with a 'content' field.
    """
    role: Optional[str] = Field(None, description="Role of the message author, e.g., 'assistant'.")
    content: Optional[str] = Field(None, description="The actual message text output by the model.")
    retrieval_context: Optional[List[SourceNodeSchema]]  = Field(None, description="The retrieval context for the message.")
    # If your system calls tools or returns function calls, you can add optional fields here.


class ChatCompletionChoice(BaseModel):
    """
    An individual choice within 'choices'. 
    Each choice usually has an index, a message, and a finish_reason.
    """
    index: int = Field(..., description="The index of this choice in the list of choices.")
    message: ChatCompletionMessage = Field(..., description="The chat message generated by the model.")
    finish_reason: Optional[str] = Field(
        None, 
        description="Why the model stopped generating tokens (e.g. 'stop', 'length', 'tool_calls', etc.)."
    )
    error: Optional[str] = Field(
        None, 
        description="Error message if the model failed to generate a choice."
    )
    logprobs: Optional[ChatCompletionLogProbs] = Field(
        None, 
        description="Log probability info for the choice, if requested."
    )


class TokenDetails(BaseModel):
    """
    Example structure for token breakdown details (if you want granular data).
    Expand with fields relevant to your system, e.g., tokens, offset, etc.
    """
    tokens: Optional[List[str]] = None
    # You could add more detail about each token.


class ChatCompletionUsage(BaseModel):
    """
    Tracks usage of tokens for prompt and completion, and possibly
    includes detailed breakdowns if your system provides it.
    """
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

    # Optional breakdowns for fine-grained analysis
    prompt_tokens_details: Optional[TokenDetails] = None
    completion_tokens_details: Optional[TokenDetails] = None


# -------------------------
# 2. Main Response Model
# -------------------------

class ChatCompletionResponse(BaseModel):
    """
    Represents the entire Chat Completion response object.
    Includes optional fields for 'service_tier', 'system_fingerprint', etc.
    """
    id: str = Field(..., description="A unique identifier for this chat completion.")
    object: str = Field("chat.completion", description="The object type, always 'chat.completion'.")
    created: int = Field(..., description="Unix timestamp (in seconds) when this completion was created.")
    model: str = Field(..., description="The model used for generating this chat completion.")
    choices: List[ChatCompletionChoice] = Field(..., description="List of completion choices.")
    usage: Optional[ChatCompletionUsage] = Field(None, description="Token usage statistics for the request.")
    
    # Additional optional fields from the spec
    service_tier: Optional[str] = Field(
        None,
        description=(
            "The service tier used for processing the request (e.g. 'default', 'auto', or 'scale')."
        )
    )
    system_fingerprint: Optional[str] = Field(
        None,
        description=(
            "Represents the backend configuration fingerprint. "
            "Used with 'seed' to diagnose determinism changes."
        )
    )




# Streaming Response Model

from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

class ChatStreamChoiceDelta(BaseModel):
    """
    A partial "delta" for a single choice.
    The 'content' might be a partial piece of text. 
    The 'role' typically only appears at the start (if at all).
    """
    role: Optional[str] = None
    content: Optional[str] = None
    # If the model calls a function or tool, you could add extra fields.

class ChatStreamChoice(BaseModel):
    """
    Each choice can include a 'delta' 
    plus an optional finish_reason when it ends.
    """
    index: int
    delta: ChatStreamChoiceDelta
    finish_reason: Optional[str] = None

class ChatCompletionChunk(BaseModel):
    """
    Represents the shape of a *single chunk* in streaming mode.
    """
    id: str
    object: str = Field("chat.completion.chunk",
                        description="Always 'chat.completion.chunk' for streaming.")
    created: int
    model: str
    # Optional extra fields
    service_tier: Optional[str] = None
    system_fingerprint: Optional[str] = None

    choices: List[ChatStreamChoice]
    
    # Will typically be null for intermediate chunks. 
    # The final chunk can include usage for the entire request.
    usage: Optional[ChatCompletionUsage] = None
